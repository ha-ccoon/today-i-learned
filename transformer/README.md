## ğŸ“š Transformer í•™ìŠµ ëª©ì°¨

ì´ í´ë”ëŠ” `How Transformer LLMs Work` ê°•ì˜ë¥¼ ë“£ê³  ì •ë¦¬í•œ ë‚´ìš© ì…ë‹ˆë‹¤.
https://www.deeplearning.ai/short-courses/how-transformer-llms-work/

## ëª©ì°¨

### 1. [Bag of Words](./1.bag-of-words.md)

- Bag of Words(BoW) ê°œë…ê³¼ ì •ì˜
- BoW í”„ë¡œì„¸ìŠ¤ (Input â†’ Tokenization â†’ Vocabulary â†’ Vector Representation)
- BoW ì˜ˆì‹œì™€ ë²¡í„° í‘œí˜„ ë°©ë²•
- ìš©ì–´ ì •ë¦¬ (Vocabulary)

### 2. [Word2Vecì™€ Word Embedding](./2.word-2-vec.md)

- Word2Vec ê°œë…ê³¼ ì‹ ê²½ë§ êµ¬ì¡°
- Word2Vecì˜ Embedding ì›ë¦¬
- Word Embeddingê³¼ ì°¨ì›(Dimension) ì„¤ëª…
- ë‹¤ì–‘í•œ ì„ë² ë”© ì¢…ë¥˜ì™€ Representation Model

### 3. [Encoding & Decoding Context with Attention](./3.encoding-and-decoding-context-with-attention.md)

- Word2Vecì˜ í•œê³„ì™€ ë¬¸ë§¥(Context) ì¸ì½”ë”©
- Autoregressive Decoderì™€ ë²ˆì—­ ê³¼ì •
- Encoder-Decoder êµ¬ì¡°ì˜ ë‹¨ê³„ë³„ ì„¤ëª…
- Attention ë©”ì»¤ë‹ˆì¦˜ì˜ ë„ì…ê³¼ ë™ì‘ ì›ë¦¬

### 4. [Transformersì™€ í˜„ëŒ€ NLP ëª¨ë¸](./4.transformers.md)

- Transformerì˜ ë“±ì¥ê³¼ êµ¬ì¡°
- Transformer Encoder/Decoderì˜ ë™ì‘ ì›ë¦¬
- BERT: Bidirectional Encoder Representations
- Representation Modelì˜ í™œìš© (Pre-train, Fine-tune)
- Generative Model (GPT ë“±)
- Context Lengthì˜ í•œê³„
- ì£¼ìš” LLM ë¹„êµ

### 5. [Transformer LLM Architectural Overview](./5.architectural-overview.md)

- Transformer LLMì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ (Tokenizer, Transformer Blocks, LM Head)
- Transformer Blocksì˜ êµ¬ì¡°ì™€ ì—­í• 
- Self-Attentionì˜ ì„¸ë¶€ ë™ì‘ (Coreference Resolution, Relevance Scoring, Combining Information)

### 6. [Self-Attentionê³¼ Transformer Blockì˜ ì„¸ë¶€ êµ¬ì¡°](./6.self-attention.md)

- Transformer Blockì˜ êµ¬ì¡°
- Self-Attentionì˜ ì—­í• ê³¼ ë™ì‘
- Self-Attentionì˜ ë‚´ë¶€ ì—°ì‚° (Query, Key, Value)
- Multi-Head Attention(MHA)ì™€ ë³‘ë ¬ ì²˜ë¦¬
- Sparse Attentionì˜ ê°œë…ê³¼ ë°©ì‹
- Transformer Blockì˜ ì£¼ìš” í•˜ì´í¼íŒŒë¼ë¯¸í„°

### 7. [ìµœì‹  íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ ë³€í™”](./7.recent-improvements.md)

- íŠ¸ëœìŠ¤í¬ë¨¸ êµ¬ì¡°ì˜ ë³€í™” (Positional Encoding, Grouped-query attention, Residual Connection)
- Training LLMs: ë°ì´í„° íš¨ìœ¨ê³¼ í¬ì§€ì…”ë„ ì„ë² ë”©
- Rotary Positional Embedding(RoPE)
- Packing ë°©ì‹ê³¼ Naive ë°©ì‹ ë¹„êµ

---
